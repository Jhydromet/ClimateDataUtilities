library(tidyverse)
library(plotly)
library(lubridate)

# Search for the symbol #!!!# to locate code that needs to be adapted for new sites


wd <- ' '

setwd(wd)

#!!!# Which site & Project are you working on?

sitecode <-  " "
projectcode <- " "

# Read in data and organize for analysis ----------------------------------

# Read in the data, fix column names

data <-  read_csv(' ',
                 skip =1,
                 col_names = c("TIMESTAMP","WaterLevel_m","WaterTemp_C")) %>% 
  mutate(TIMESTAMP = dmy_hms(TIMESTAMP))

# Quick plot if you want

p <- data %>% 
  pivot_longer(!TIMESTAMP) %>% 
  ggplot()+
  geom_line(aes(TIMESTAMP,value))+
  facet_grid(vars(name))

#p
#ggplotly(p)

# Split temperature and level data -------------------------------------------------------

long.data <- data %>% 
  pivot_longer(!TIMESTAMP, names_to = "parameter", values_to = "value_raw")

tempC.data <- filter(long.data, parameter == "WaterTemp_C")

levelm.data <- filter(long.data, parameter == "WaterLevel_m")


# ------------------------------------------------------------------------------
# Data QAQC by parameter ----------------------------------------------
# ------------------------------------------------------------------------------

# Temperature QAQC --------------------------------------------------------

#badtempintervals <-  list(interval(ymd_hms("2021-08-03 15:15:00"),ymd_hms("2021-10-06 11:30:00")))

corrected.tempC.data <- tempC.data %>% 
  mutate(
    grade  = case_when(
      is.na(value_raw) | is.nan(value_raw) ~ -2, # GradesNAs or NaNs in data
      #TIMESTAMP %within% badtempintervals ~ -2, # Grades if in pior defined bad data intervals
      TRUE ~ 51),
    approval  = 3,
    value_corrected = value_raw)

# Water Level QAQC --------------------------------------------------------

#!!!# Level offsets to apply?

# This section of code applies offsets and creates the values_corrected column

# OFFSET 1

# Survey on 6 October 2021 shows benchmarks BM1 & BM4 have not moved, while the top of staff gauge and top of logger rod have subsided by 0.022 and 0.018 m respectively since the previous survey on 02 August 2021. Therefore an offset of 0.02 m is being applied on data from 6 Oct 2021 forward. Additionally, ice affected data on 19 October and from 30 October onward has been omitted.


# INPUT VALUES
survey1.date <- ymd_hms("2021-08-09 20:45:00")
survey2.date <- ymd_hms("2021-10-06 17:15:00")
timestep <- "15 min"
total.offset <- 0.02

# Create a cumulative offset dataframe

offset.values <- tibble(TIMESTAMP = seq.POSIXt(from = survey1.date, 
                                               to = survey2.date, 
                                               by = timestep),
                        incremental.offset = total.offset/length(TIMESTAMP),
                        cusum.offset = cumsum(incremental.offset))

# Filter the level data and split the data into pre, during, and post offset phases

# pre.offset data has no corrections
pre.offset <- levelm.data %>%
  filter(TIMESTAMP < survey1.date) %>% 
  mutate(
    value_corrected = value_raw,
    grade = 31,
    approval = 3)

# This has the cumulative offset calculation for the corrected values
incremented.offset <- levelm.data %>%
  filter(TIMESTAMP %in% offset.values$TIMESTAMP) %>% 
  mutate(
    value_corrected = value_raw + offset.values$cusum.offset,
    grade = 30,
    approval = 3)

# This has a fixed offset 
post.offset <- levelm.data %>% 
  filter(TIMESTAMP >survey2.date) %>% 
  mutate(
    value_corrected = value_raw + tail(offset.values$cusum.offset,n =1),
    grade = 31,
    approval = 3)

# Bind the corrected level data back together
offset.levelm.data <- bind_rows(pre.offset,incremented.offset, post.offset)


#!!!# What are the bad data intervals? e.g. ice effected, dry.. create separate intervals for each

iceintervals <- list(
  interval(ymd_hms("2021-10-30 01:45:00"),ymd_hms("2022-03-01 00:00:00")),
  interval(ymd_hms("2021-10-19 08:15:00"), ymd_hms("2021-10-19 13:15:00")),
  interval(ymd_hms("2021-10-19 08:15:00"), ymd_hms("2021-10-19 13:15:00"))
)

corrected.levelm.data <- offset.levelm.data %>%
  mutate(
    grade = case_when(
      is.na(value_raw) | is.nan(value_raw) ~ -2, #Grades NAs or NaNs in data
      TIMESTAMP %within% iceintervals ~ 3, # Grades if in prior defined bad data intervals
      TRUE ~ grade),  # keep grades a previously defined otherwise
    approval  = 3,
    ) 


# re-join temp and level data

corrected.data <- full_join(corrected.levelm.data, corrected.tempC.data)


# Final Data Export -------------------------------------------------------

final.data <- corrected.data %>% 
  rename(DateTime = "TIMESTAMP") %>% 
  mutate(
    ProjectID = projectcode,
    StationID = sitecode,
    value_raw = na_if(value_raw, "NaN"),
    value_corrected = case_when(
      grade >= 11 ~ as.character(value_corrected), # set corrected value to NA if not passing QAQC
      TRUE ~ "NA"),
    grade_label = case_when(
      grade == -2	~ "Unusable data",
      grade == -1	~ "Unspecified",
      grade == 0	~ "Undefined",
      grade == 1	~ "Unverified data",
      grade == 2 ~	"Dry conditions affect record",
      grade == 3 ~	"Ice conditions affect record",
      grade == 4 ~	"Incomplete or partial aggregated record",
      grade == 5 ~	"Estimated data with No confidence",
      grade == 10 ~	"Estimated data with Poor confidence",
      grade == 11	~ "Poor data",
      grade == 20 ~	"Estimated data with Fair confidence",
      grade == 21	~ "Fair data",
      grade == 30 ~	"Estimated data with Good confidence",
      grade == 31 ~	"Good data",
      grade == 40 ~	"Estimated data with Very Good confidence",
      grade == 41 ~	"Very Good data",
      grade == 50 ~	"Estimated data with Excellent confidence",
      grade == 51 ~	"Excellent data"),
    approval_label = case_when(
      approval == 0	~ "Undefined",
      approval == 1	~ "Working",
      approval == 2	~ "In Review",
      approval == 3	~ "Approved")) %>% 
  select(ProjectID,	StationID,	DateTime,	parameter,	
         value_raw,	value_corrected,	grade,	grade_label,	approval,	approval_label)


# Plot to check

p <- final.data %>%
  filter(grade >11) %>% 
  ggplot()+
  geom_line(aes(DateTime,value_raw), linetype = "dashed")+
  geom_line(aes(DateTime,as.double(value_corrected)))+
  geom_point(aes(DateTime, as.double(value_corrected), colour = as.factor(grade)))+
  scale_colour_discrete(name = "Grades")+
  scale_y_continuous(name = element_blank())+
  facet_grid(rows = vars(parameter),scales = "free")

ggplotly(p)
p
ggsave(p, filename = "Offset_E3(H).png", device = "png")

export <- final.data %>% 
  mutate(across(.cols = everything(), as.character)) 

#!!!# Where is this being exported?

export.dest <- " "

write_csv(export.final, path = export.dest)


# Summary Statistics ------------------------------------------------------


data.summary <- Graded.dat %>% 
  filter(grade >= 10) %>% # this drops all the error Graded data
  group_by(parameter) %>% 
  mutate(value = na_if(value_raw, NaN)) %>% 
  mutate(mean.value = mean(value_raw, na.rm = T),
         max.value = max(value_raw, na.rm = T),
         min.value = min(value_raw, na.rm = T),
         date = date(TIMESTAMP)) %>%
  select(!TIMESTAMP) %>% 
  filter(value_raw == max.value | value_raw == min.value) %>% 
  distinct()

# Summary Export

write_csv(data.summary, " ")


